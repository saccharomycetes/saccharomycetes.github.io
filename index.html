<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jerry (Jiarui) Zhang</title>

    <meta name="author" content="Jiarui Zhang">
    <meta name="author" content="Jerry Zhang">
    <meta name="keywords" content="Jerry Zhang, Jiarui Zhang">
    <meta name="description" content="Jiarui Zhang's personal website">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/advanced_ai_robot_icon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <meta name="google-site-verification" content="crFyq-qgHJiEAi2T6GPvkGqU-VRiQ3odY7QtFYUV52k" />
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Jerry (Jiarui) Zhang
                </p>
                <p>I am a second-year Computer Science Ph.D. student at <a href="https://www.cs.usc.edu/">USC</a>. Previously, I received my bachelor's degree in electrical engineering from <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>. I grew up in Dalian, a beautiful coastal city in China. My Chinese name is Âº†ÂÆ∂Áëû.
                </p>
                <p>
                  My ambitious dream is to develop an AI that can observe and reason over the real physical world. One of the motivation is from the fact that <a href="https://arxiv.org/pdf/1712.01815.pdf"> AlphaGo Zero</a> beats the world champion in Go game without learning from any human annotation, which means human's top strategy is far from optimal in a well-defined decision space with a clear goal. In the complex real world, I believe AI will be able to bring us much more surprises.
                </p>
                <p style="text-align:center">
                  <a href="mailto:jzhang37@usc.edu">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=rM4hgN8AAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/JiaruiZ58876329">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/saccharomycetes/">Github</a> &nbsp;/&nbsp;
                  <a href="https://saccharomycetes.github.io/images/wechat.png">Wechat</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/Jerry.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/Jerry.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <p style="color: rgb(20, 25, 158);">
                  I'm very happy to chat about research ideas and collaborate with people. Please feel free to reach out to me if you are interested in discussing or working together!
                  <!-- Representative papers are <span class="highlight">highlighted</span>. -->
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                
                <h2>Research</h2>
                <p>
                  Currently, my research focuses on studying multimodal LLMs' (MLLM) properties (response to visual details, bias in object locations, calibration) and capabilities (VQA, nonverbal reasoning). I am also interested in developing new MLLMs by introducig new architecture and tasks.
                  <!-- Representative papers are <span class="highlight">highlighted</span>. -->
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


            <tr>
              <td style="padding:10px;width:50%;vertical-align:middle">
                  <img src='images/perceptual_limitation.png' width=100%>
              </td>
              <td style="padding:10px;width:50%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2402.07384.pdf">
                  <span class="papertitle">Exploring Perceptual Limitation of Multimodal Large Language Models</span>
                </a>
                <br>
                <strong>Jiarui Zhang*</strong>,
                <a href="https://jameshujy.github.io/">Jinyi Hu*</a>,
                <a href="https://mahyarkoy.github.io/">Mahyar Khayatkhoei</a>,
                <a href="https://www.ilievski.info/">Filip Ilievski</a>,
                <a href="https://www.cs.tsinghua.edu.cn/csen/info/1180/4033.htm">Maosong Sun</a>
                <!-- <a href="https://pratulsrinivasan.github.io/nuvo/">project page</a>
                /
                <a href="https://www.youtube.com/watch?v=hmJiOSTDQZI">video</a>
                / -->
                <br>
                <a href="https://arxiv.org/pdf/2402.07384.pdf">arXiv</a>, 
                <a href="https://github.com/saccharomycetes/mllm-perceptual-limitation">Github</a>
                <p></p>
                <p>
                  We expose a limitation of several state-of-the-art multimodal LLMs in perceiving small visual objects. 
                  Then we identify four factors that influence this limitation, namely, object quality, size, distractor, and location. Through controlled intervention studies, we reveal the distinct impact caused by each factor. Our findings will potentially offer insights to improve visual processing capabilities of MLLMs.
                </p>
              </td>
            </tr>

    <tr>
      <td style="padding:10px;width:30%;vertical-align:middle">
          <img src='images/vicrop.png' width=100%>
      </td>
      <td style="padding:10px;width:70%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2310.16033.pdf">
          <span class="papertitle">Visual Cropping Improves Zero-Shot Question Answering of Multimodal Large Language Models</span>
        </a>
        <br>
        <strong>Jiarui Zhang</strong>,
        <a href="https://mahyarkoy.github.io/">Mahyar Khayatkhoei</a>,
        <a href="https://www.prateekchhikara.com/">Prateek Chhikara</a>,
        <a href="https://www.ilievski.info/">Filip Ilievski</a>
        <br>
        <em>NeurIPS R0-FoMo Workshop</em>, 2023
        <br>
        <!-- <a href="https://pratulsrinivasan.github.io/nuvo/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=hmJiOSTDQZI">video</a>
        / -->
        <a href="https://arxiv.org/pdf/2310.16033.pdf">arXiv</a>, 
        <a href="https://github.com/saccharomycetes/visual_crop_zsvqa">Github</a>
        <p></p>
        <p>
          We qualitatively and quantitatively show the limitation of two state-of-the-art multimodal LLMs (MLLMs) in perceiving small visual details for zero-shot visual question answering. Then we found out this is mitigatable by visual cropping following internal attention of MLLM.
        </p>
      </td>
    </tr>


    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:10px;width:30%;vertical-align:middle">
          <img src='images/KDD-pipeline.png' width=100%>
      </td>
      <td style="padding:10px;width:70%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2306.02520.pdf">
          <span class="papertitle">A Study of Situational Reasoning for Traffic Understanding</span>
        </a>
        <br>
        <strong>Jiarui Zhang</strong>,
        <a href="https://www.ilievski.info/">Filip Ilievski</a>,
        <a href="https://mayer123.github.io/">Kaixin Ma</a>,
        <a>Aravinda Kollaa</a>,
        <a href="https://scholar.google.com/citations?user=7CLS0LwAAAAJ&hl=en">Jonathan Francis</a>,
        <a href="https://www.bosch.com/research/about-bosch-research/our-research-experts/alessandro-oltramari/">Alessandro Oltramari</a>,
        
        <br>
        <em>KDD</em>, 2023
        <br>
        <!-- <a href="https://pratulsrinivasan.github.io/nuvo/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=hmJiOSTDQZI">video</a>
        / -->
        <a href="https://arxiv.org/pdf/2306.02520.pdf">arXiv</a>, 
        <a href="https://github.com/saccharomycetes/text-based-traffic-understanding">Github</a>
        <p></p>
        <p>
          We formalize three novel text-based benchmarks on traffic domain, including decision making, real and hypothetical events casual reasoning, and knowledge testing. Then we study the ability of diverse knowledge-enhanced language models on our benckmarks.
        </p>
      </td>
    </tr>

    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:10px;width:30%;vertical-align:middle">
          <img src='images/kcap.png' width=100%>
      </td>
      <td style="padding:10px;width:70%;vertical-align:middle">
        <a href="https://dl.acm.org/doi/pdf/10.1145/3587259.3627561">
          <span class="papertitle">Knowledge-enhanced Agents for Interactive Text Games</span>
        </a>
        <br>
        <a href="https://www.prateekchhikara.com/">Prateek Chhikara</a>,
        <strong>Jiarui Zhang</strong>,
        <a href="https://www.ilievski.info/">Filip Ilievski</a>,
        <a href="https://scholar.google.com/citations?user=7CLS0LwAAAAJ&hl=en">Jonathan Francis</a>,
        <a href="https://mayer123.github.io/">Kaixin Ma</a>,
        
        <br>
        <em>International Conference on Knowledge Capture   (KCap)</em>, 2023, <br>
        <span style="color: red;">üèÜüèÜ Best Student Paper Award üèÜüèÜ </span>
        <br>
        <!-- <a href="https://pratulsrinivasan.github.io/nuvo/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=hmJiOSTDQZI">video</a>
        / -->
        <p></p>
        <p>
          We introduces a knowledge-injection framework to enhance the functional grounding of agents in text-based games, addressing existing limitations in coherence, contextual awareness, and learning. The framework employs strategies like knowledge graphs and input encoding augmentations. Tested on 10 tasks in the ScienceWorld environment, the study reveals how task properties, model architectures, and domain knowledge interact in interactive contexts.
        </p>
      </td>
    </tr>

    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:10px;width:30%;vertical-align:middle">
          <img src='images/AKBC22.png' width=100%>
      </td>
      <td style="padding:10px;width:70%;vertical-align:middle">
        <a href="https://www.akbc.ws/2022/assets/pdfs/3_a_study_of_zero_shot_adaptatio.pdf">
          <span class="papertitle">A Study of Zero-shot Adaptation with Commonsense Knowledge</span>
        </a>
        <br>
        <strong>Jiarui Zhang</strong>,
        <a href="https://www.ilievski.info/">Filip Ilievski</a>,
        <a href="https://mayer123.github.io/">Kaixin Ma</a>,
        <a href="https://scholar.google.com/citations?user=7CLS0LwAAAAJ&hl=en">Jonathan Francis</a>,
        <a href="https://www.bosch.com/research/about-bosch-research/our-research-experts/alessandro-oltramari/">Alessandro Oltramari</a>,
        
        <br>
        <em>AKBC</em>, 2022
        <br>
        <!-- <a href="https://pratulsrinivasan.github.io/nuvo/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=hmJiOSTDQZI">video</a>
        / -->
        <a href="https://www.akbc.ws/2022/assets/pdfs/3_a_study_of_zero_shot_adaptatio.pdf">arXiv</a>, 
        <a href="https://github.com/saccharomycetes/commonsense-with-KG">Github</a>
        <p></p>
        <p>
          We train different sizes of language models using synthetic data from knowledge graphs. We observe significant zero-shot performance improvement different language tasks. We also study the effect of knowledge graph training data size and find out more data does not always lead to better performance, and the optimal data size grows with the model size.
        </p>
      </td>
    </tr>
	
	
    <!-- <tr onmouseout="recon_stop()" onmouseover="recon_start()" bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='recon_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/recon.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/recon.png' width="160">
        </div>
        <script type="text/javascript">
          function recon_start() {
            document.getElementById('recon_image').style.opacity = "1";
          }

          function recon_stop() {
            document.getElementById('recon_image').style.opacity = "0";
          }
          recon_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://reconfusion.github.io/">
			<span class="papertitle">ReconFusion: 3D Reconstruction with Diffusion Priors</span>
        </a>
        <br>
        <a href="https://www.cs.columbia.edu/~rundi/">Rundi Wu*</a>,
		<a href="https://bmild.github.io/">Ben Mildenhall*</a>,
        <a href="https://henzler.github.io/">Philipp Henzler</a>,
        <a href="https://keunhong.com/">Keunhong Park</a>,
        <a href="https://ruiqigao.github.io/">Ruiqi Gao</a>,
        <a href="https://scholar.google.com/citations?user=_pKKv2QAAAAJ&hl=en/">Daniel Watson</a>,
        <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>,
        <a href="https://dorverbin.github.io/">Dor Verbin</a>,
		<strong>Jonathan T. Barron</strong>,
        <a href="https://poolio.github.io/">Ben Poole</a>,
        <a href="https://holynski.org/">Aleksander Holynski*</a>
        <br>
        <em>arXiv</em>, 2023
        <br>
        <a href="https://reconfusion.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/">arXiv</a>
        <p></p>
        <p>
        Using a multi-image diffusion model as a regularizer lets you recover high-quality radiance fields from just a handful of images.
        </p>
      </td>
    </tr> -->

          </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
                <p>
                  I enjoy weight lifting in my free time.
                </p>
                <p>I also enjoy cooking recently.
                </p>
                <p>I like eating burgers.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
            <!-- <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
              <td width="75%" valign="center">
                <a href="https://cvpr.thecvf.com/Conferences/2024/Organizers">Area Chair, CVPR 2024</a>
                <br>
                <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
                <br>
                <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                <br>
                <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Award Committee Member, CVPR 2021</a>
                <br>
                <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                <br>
                <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/cs188.jpg" alt="cs188">
              </td>
              <td width="75%" valign="center">
                <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
                <br>
                <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
                <br>
                <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
              </td>
            </tr>
            

            <tr>
              <td align="center" style="padding:20px;width:25%;vertical-align:middle">
                <h2>Basically <br> Blog Posts</h2>
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr> -->

            
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  This website is adapted from <a href="https://github.com/jonbarron/jonbarron_website"> here</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
